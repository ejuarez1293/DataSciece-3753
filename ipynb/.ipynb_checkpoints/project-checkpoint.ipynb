{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from datetime import date\n",
    "import calendar\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import math\n",
    "from IPython.display import display, HTML\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from scipy import stats\n",
    "%matplotlib inline\n",
    "\n",
    "cd = os.path.split(os.getcwd())[0]\n",
    "if cd not in sys.path:\n",
    "    sys.path.append(cd)\n",
    "\n",
    "from lib import noaa, bexarcrime\n",
    "\n",
    "\n",
    "# Set this to true if you want to run it from scratch\n",
    "# IE pulling all the data from source and running all the slow\n",
    "# functions\n",
    "PROCESS_FULLY = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents <a class=\"anchor\" id=\"toc\"></a>\n",
    "* [City Selection](#selection)\n",
    "  * [Loading](#loading)\n",
    "  * [Merging](#merging)\n",
    "  * [Visualisations](#visualisations)\n",
    "* [Data Acquisition](#acquisition)\n",
    "* [Data Analysis](#analysis)\n",
    "  * [Hypthesis](#hypothesis)\n",
    "  * [Exploration](#exploration)\n",
    "* [Results and Conclusions](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## City Selection <a class=\"anchor\" id=\"selection\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to find which cities to examine. We loaded datasets full of factors that we thought influenced violent crime at the county level, and then chose the county seats of those counties to examine in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Datasets <a class=\"anchor\" id=\"loading\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load the datasets into dataframes, later on we'll merge them into a single one to examine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "County-level crime dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using crime reports, not arrests \n",
    "crime = pd.read_csv('../data/CountyCrimeReports.tsv', sep='\\t')\n",
    "crime['FIPS'] = crime['FIPS_ST'] * 1000 + crime['FIPS_CTY']\n",
    "crime['vcrime'] = crime['MURDER'] + crime['RAPE'] + crime['ROBBERY'] + crime['AGASSLT']\n",
    "crime = crime.set_index('FIPS')\n",
    "crime = crime[['COVIND', 'vcrime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Education dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edu = pd.read_excel('../data/Education.xls', skiprows=4)\n",
    "\n",
    "# state and areas are named nicely in this dataset and will be kept for the later 'join'\n",
    "# columns[-4:] include most recent data for adults eduction\n",
    "# I chose the most recent because its not like the total number of HS dropouts is going to change THAT much\n",
    "edu = edu[['FIPS Code', 'State', 'Area name'] + list(edu.columns[-4:])]\n",
    "edu.rename(columns={'FIPS Code':'FIPS', \\\n",
    "                    'Area name':'County',\\\n",
    "                    'Percent of adults with less than a high school diploma, 2011-2015':'p_no_HS_dip', \\\n",
    "                    'Percent of adults with a high school diploma only, 2011-2015':'p_HS_dip',\\\n",
    "                    'Percent of adults completing some college or associate\\'s degree, 2011-2015':'p_some_college',\\\n",
    "                    'Percent of adults with a bachelor\\'s degree or higher, 2011-2015':'p_college_dip'}, inplace=True)\n",
    "edu = edu.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Population dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop = pd.read_excel('../data/PopulationEstimates.xls', skiprows=2)\n",
    "\n",
    "# average the columns\n",
    "cols = ['POP_ESTIMATE_2010','POP_ESTIMATE_2011','POP_ESTIMATE_2012','POP_ESTIMATE_2013','POP_ESTIMATE_2014','POP_ESTIMATE_2015','POP_ESTIMATE_2016']\n",
    "pop['avgpop'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "# more averaging\n",
    "cols = ['N_POP_CHG_2010','N_POP_CHG_2011','N_POP_CHG_2012','N_POP_CHG_2013','N_POP_CHG_2014','N_POP_CHG_2015','N_POP_CHG_2016']\n",
    "pop['dpop/dt'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "# only pull FIPS code, population, and dp\n",
    "pop = pop[['FIPS', 'avgpop', 'dpop/dt']]\n",
    "pop = pop.set_index('FIPS')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poverty estimate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pov = pd.read_excel('../data/PovertyEstimates.xls', skiprows=3)\n",
    "# only select poverty percentage\n",
    "pov = pov[['FIPStxt', 'PCTPOVALL_2015']]\n",
    "pov.rename(columns={'FIPStxt':'FIPS', 'PCTPOVALL_2015':'p_impoverished'}, inplace=True)\n",
    "pov = pov.set_index('FIPS')\n",
    "pov.p_impoverished = pd.to_numeric(pov.p_impoverished, errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employment estimates dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp = pd.read_excel('../data/Unemployment.xls', skiprows=9)\n",
    "\n",
    "#avg unemployment\n",
    "cols = ['Unemployment_rate_2007', 'Unemployment_rate_2008', 'Unemployment_rate_2009', 'Unemployment_rate_2010', 'Unemployment_rate_2011', 'Unemployment_rate_2012', 'Unemployment_rate_2013', 'Unemployment_rate_2014', 'Unemployment_rate_2015', 'Unemployment_rate_2016']\n",
    "emp['p_unempl'] = emp[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "#only pull average and income\n",
    "emp = emp[['FIPStxt', 'p_unempl', 'Median_Household_Income_2015']]\n",
    "emp.rename(columns={'FIPStxt':'FIPS', 'Median_Household_Income_2015':'med_income'}, inplace=True)\n",
    "emp = emp.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging the Datasets<a class=\"anchor\" id=\"merging\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the datasets into a single one, indexed on the FIPS code. We remove the country- and state-level information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = edu.join([pop,pov,emp,crime], how='outer')\n",
    "df = df.where(df.State != 'PR').dropna(how='all') ## Puerto Rico has unreliable data\n",
    "\n",
    "#pull out nationwide data\n",
    "us = df.iloc[0]\n",
    "df = df.drop(0)\n",
    "\n",
    "#pull out statewide data\n",
    "s = [x for x in range(1000,75000,1000)]\n",
    "states = df.loc[s].dropna(how='all')\n",
    "\n",
    "# all thats left is county level data\n",
    "df = df.drop(states.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to normalize the change in population and violent crime rate. In this analysis, violent crime is expressed in crimes per 100,000 residents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "df['p_dpop'] = df['dpop/dt']/df['avgpop']\n",
    "df['vcrime_rate'] = 100000 * df['vcrime']/df['avgpop']\n",
    "df = df.drop(['dpop/dt', 'vcrime'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualising the Data <a class=\"anchor\" id=\"visualisations\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to visualise the county-level data set to see if there are any interesting patterns. First we'll look at the basic statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll compare some factors of different states, with bonus political standpoints according the the 2008 presidential election between Barack Obama and John McCain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot of violent crime rate per state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# According to the 2008 presidential election\n",
    "blue_states =['WA', 'OR', 'CA', 'NV', 'NM', 'CO', 'MN', 'IA', 'WI', 'IL', 'IN', 'MI', 'OH', 'PA', 'NY', 'VT', 'NH', 'ME', 'MA', 'CT', 'RI', 'NJ', 'DE', 'MD', 'VA', 'NC', 'FL', 'HI']\n",
    "red_states = ['ID', 'MT', 'WY', 'UT', 'AZ', 'ND', 'SD', 'NE', 'KS', 'OK', 'TX', 'MO', 'AR', 'LA', 'WV', 'KY', 'TN', 'MS', 'AL', 'GA', 'SC', 'AK']\n",
    "fix, ax = plt.subplots(figsize=(20,10))\n",
    "pal = {state: 'r' if state in red_states else \"b\" for state in df.State}\n",
    "sns.boxplot(ax=ax, x='State', y='vcrime_rate', data=df, palette=pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box plot of percentages without a highschool diploma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "sns.boxplot(ax=ax, x='State', y='p_no_HS_dip', data=df, palette=pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs of factors to violent crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, y_vars=['vcrime_rate'], x_vars=['p_no_HS_dip', 'p_HS_dip', 'p_some_college', 'p_college_dip', 'avgpop',\n",
    "   'p_impoverished', 'p_unempl', 'med_income', 'p_dpop', 'vcrime_rate'], dropna=True, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent crime per 100,000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.avgpop.dropna().apply(np.log10), axlabel=\"Population (log10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.p_unempl.dropna(), axlabel='Unemployment Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.p_impoverished.dropna(), axlabel=\"Poverty Rate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to select counties that have similar violent crime factors. For example, we don't want to end up comparing a well-to-do city in the suburbs of Maine against skid-row L.A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we bin the data into high, medium and low (based on national quantiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binned = pd.DataFrame({c : pd.qcut(df[c], 3, labels=['L', 'M', 'H']) for c in df.drop(['State', 'County', 'COVIND'], axis=1).columns}).join(df[['State', 'County', 'COVIND']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a table of the five worst counties in Texas (based on high rates of unemployment, crime, and population. Half of them are border towns with immigration problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TX = binned.dropna(how='all').groupby(['vcrime_rate', 'p_unempl', 'avgpop'])\n",
    "display(df.loc[TX.get_group(('H', 'H', 'H')).index].where(df.State == 'TX').dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graph of how Texas matches up nationwide to crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sns.distplot(df.where(df.State=='TX').vcrime_rate.dropna(), label=\"Violent Crime Rates in Texas\")\n",
    "sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent Crime Rates in US and Texas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table of the highest crime rates in the US with at least a population of 10000 to cull outliers. \n",
    "\n",
    "Note that high city crime does not necessarily match high county crime. For example, Chicago is a high crime city, but because it's split between two counties it's ranked lower on this list. St. Louis, however, is both a city and its own county, so it's data is more precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "display(df.where(df.avgpop > 10000).sort_values('vcrime_rate', ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We grouped the data by violent crime rate, poverty rate, unemployment rate, and population.\n",
    "\n",
    "We select counties with high rates of enemployment, violent crime, poverty, and large populations sampled using a nonrandom seed for consistency between runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all counties grouped by H/M/L rates of each factor\n",
    "groups = ['vcrime_rate', 'p_impoverished', 'p_unempl', 'avgpop']\n",
    "\n",
    "c = binned.dropna(how='all').groupby(groups[::-1])\n",
    "display(c.count().where(c.count().State > 10).dropna().sort_values('State', ascending=False)['State'].unstack())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "selection = ('H','H','H', 'H')\n",
    "for x in groups:\n",
    "    print(\"%10s \" %x[:10], end='')\n",
    "print('')\n",
    "for x in selection:\n",
    "    print(\"%10s \" %x[:10], end='')\n",
    "HHHstates = df.loc[c.get_group(selection).index]\n",
    "display(HHHstates.where(HHHstates.vcrime_rate > 800).dropna().sample(10, random_state=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those counties, we selected the county seats as the cities\n",
    "We looked up the latitude and longitude of them to match to NOAA's list of weather stations. We used Pythagoras' theorem to find the closest station to the city, as some cities may not have one within city limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = {\n",
    "    'Philidelphia, PA' : (39.9526, -75.1652), #Philadelphia County\n",
    "    'Albany, GA' : (31.5785, -84.1557), # Gougherty County\n",
    "    'Memphis, TN' : (35.1495, -90.0490), # Shelby County and Crittenden County\n",
    "    'Toledo, OH' : (41.6639, -83.5552), # Lucas County\n",
    "    'Pine Bluff AR' : (34.2284, -92.0032), # Jefferson County\n",
    "    'Detroit, MI' : (42.3314, -83.0458), # Wayne County\n",
    "    'Baltimore, MD' : (39.2904, -76.6122), # Baltimore City\n",
    "    'Flint, MI' : (43.0125, -83.6875), # Genesee County\n",
    "    'St. Louis, MO' : (38.6270, -90.1994) # St. Louis City\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we cross-referenced those locations with the NOAA ISD dataset to find the nearest stations.\n",
    "\n",
    "The stations were filtered such that we only selected stations that had recent (more recent than 2012) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of stations with location, name, and recording beginning and end dates\n",
    "hist = pd.read_csv('ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv')\n",
    "# only recent stations\n",
    "hist = hist.where(hist.END > 20120101 ).dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We defined some helper functions to process the station codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return math.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_station_code(usaf, wban):\n",
    "    usafstr = str(int(usaf))\n",
    "    wbanstr = str(int(wban))\n",
    "    \n",
    "    if len(usafstr) < 6:\n",
    "        usafstr = '0'*(6-len(usafstr)) + usafstr\n",
    "        \n",
    "    if len(wbanstr) < 5:\n",
    "        wbanstr = '0'*(5-len(wbanstr)) + wbanstr\n",
    "        \n",
    "    return usafstr + '-' + wbanstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = dict()\n",
    "for city in cities.keys():\n",
    "    coord = cities[city]\n",
    "    mindist = 999\n",
    "    minindex = 0\n",
    "    for index, row in hist.iterrows():\n",
    "        d = dist(coord, (row['LAT'], row['LON']))\n",
    "        if (d < mindist):\n",
    "            mindist = d\n",
    "            minindex = index\n",
    "    print('Nearest ({:^6.2f}) ISD to {:20} is {:40} at loc {}'.format(mindist, city, hist.loc[minindex]['STATION NAME'], minindex))\n",
    "    stations[city] = format_station_code(hist.loc[minindex]['USAF'], hist.loc[minindex]['WBAN'])\n",
    "    print('\\tStation code is {}'.format(stations[city]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Acquisition <a class=\"anchor\" id=\"acquisition\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to download the crime data for each city. We used SpotCrime to get the information, but we had to scrape their website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets define some helper functions to download crime data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_soup(city):\n",
    "    \"\"\"Returns BeautifulSoup object for each set of links\"\"\"\n",
    "    r = requests.get('https://spotcrime.com/' + city + 'daily')\n",
    "    r2 = requests.get('https://spotcrime.com/' + city + 'daily/more')\n",
    "    soup1 = BeautifulSoup(r.text, 'html.parser')\n",
    "    soup2 = BeautifulSoup(r2.text, 'html.parser')\n",
    "    \n",
    "    return soup1, soup2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(soups):\n",
    "    \"\"\"Pulls all the links from each BeautifulSoup object into a single list\"\"\"\n",
    "    links = []\n",
    "    for dates in soups[0].find_all('ol', class_='list-unstyled'):\n",
    "        for link in dates.find_all('a'):\n",
    "            links.append(link['href'])\n",
    "    for dates in soups[1].find_all('ol', class_='list-unstyled'):\n",
    "        for link in dates.find_all('a'):\n",
    "            links.append(link['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crime_df(links, base_url):\n",
    "    \"\"\"Loads each link and downloads the table of crimes, storing it in a list of lists\n",
    "    Returns a dataframe\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for i, link in enumerate(links):\n",
    "        print(i, link)\n",
    "        try:\n",
    "            r = requests.get(base_url + link)\n",
    "        except:\n",
    "            print('uh oh, timeout')\n",
    "            time.sleep(10)\n",
    "            r = requests.get(base_url+link)\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        table = soup.find('table')\n",
    "        if table == None:\n",
    "            print('no table, skipping')\n",
    "            continue\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            crime = [ele for ele in cols if ele]\n",
    "            if len(crime) == 0:\n",
    "                continue\n",
    "            if len(crime) == 4:\n",
    "                crime = ['A'] + crime\n",
    "            data.append(crime)\n",
    "    return pd.DataFrame(data, columns=['A', 'Crime', 'Time', 'Address', 'Details'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_url = 'https://spotcrime.com'\n",
    "cities = [\n",
    "    'mi/detroit/',\n",
    "    'mo/st.+louis/',\n",
    "    'md/baltimore/',\n",
    "    'oh/toledo/',\n",
    "    'ga/albany/',\n",
    "    'mi/flint/',\n",
    "    'tn/memphis/',\n",
    "    'pa/philadelphia/',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll download all the crime data for each city and save it to a gzipped csv file in the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    for city in cities:\n",
    "        soup1, soup2 = get_soup(city)\n",
    "\n",
    "        links = get_links((soup1, soup2))\n",
    "        df = crime_df(links, base_url)\n",
    "        df = df.drop(['A', 'Address', 'Details'], axis=1)\n",
    "        df.to_csv('../data/crime_{}_{}.csv.gz'.format(city[3:-1], city[:2]), compression='gzip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For ease of use, we defined a class to hold the city datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class City:\n",
    "    \"\"\"Class representing each city\n",
    "    \n",
    "    Attributes:\n",
    "        name (str): Name of the city City, 2-letter State\n",
    "        filepath (str): Filepath for the crime data\n",
    "        isd_code (str): Code for NOAA's ISD. Composed of USAF-WBAN id\n",
    "        dfc (DataFrame): Dataframe holding all crime data for the city\n",
    "        dfv (DataFrame): Dataframe holding only violent crime for the city\n",
    "        dfw (DataFrame): Dataframe holding weather data for the city\n",
    "        df (Dataframe) : Dataframe holding the merged weather+violentcrime data\n",
    "        all_crime (DataFrame): Alias for dfc\n",
    "        violent_crime (DataFrame): Alias for dfv\n",
    "        weather (Dataframe): Alias for dfw\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name, filepath, isd_code):\n",
    "        self.name = name\n",
    "        self.filepath = filepath\n",
    "        self.isd_code = isd_code\n",
    "    \n",
    "    def load_crime(self, process=True):\n",
    "        \"\"\"Reads crime dataset from filepath and stores in dfc and dfv\n",
    "        \n",
    "        Args:\n",
    "            process (bool): Whether to immediately or lazily process the data\n",
    "                Defaults to true, process the data\n",
    "        \n",
    "        Returns:\n",
    "            self\n",
    "        \"\"\"\n",
    "        self.dfc = pd.read_csv(self.filepath, compression='gzip')\n",
    "        self.dfv = self.dfc.where(self.dfc.Crime.isin(['Assault', 'Robbery', 'Shooting'])).dropna()\n",
    "        self.all_crime = self.dfc\n",
    "        self.violent_crime = self.dfv\n",
    "        if process: \n",
    "            return self.process_crime()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def process_crime(self, how='Fast'):\n",
    "        \"\"\"Processes the crime by converting the dates to datetime dtypes\n",
    "        \n",
    "        Args:\n",
    "            how (str): How to process the crime. Fast method drops data that doesn't\n",
    "                            specify the time of day, and is in general faster\n",
    "                            \n",
    "        Returns:\n",
    "            self\n",
    "            \n",
    "        \"\"\"\n",
    "        if how == 'Fast':\n",
    "            self.dfc.Time = pd.to_datetime(self.dfc.Time, \n",
    "                                           format='%m/%d/%y. %I:%M %p.', \n",
    "                                           errors='coerce').dropna()\n",
    "            self.dfc = self.dfc[self.dfc.Time.notnull()]\n",
    "        else:\n",
    "            self.dfc.Time = pd.to_datetime(self.dfc.Time, errors='coerce')\n",
    "            \n",
    "        self.dfc = self.dfc.set_index('Time')\n",
    "        self.dfv = self.dfc.where(self.dfc.Crime.isin(['Assault', 'Robbery', 'Shooting', 'Arson'])).dropna()\n",
    "        self.all_crime = self.dfc\n",
    "        self.violent_crime = self.dfv\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def load_weather(self, start=2016, end=2018):\n",
    "        \"\"\"Loads weather over FTP from NOAA's website into dfw\n",
    "        \n",
    "        Args:\n",
    "            start (int): Start year \n",
    "            end (int): End year\n",
    "            \n",
    "        Returns:\n",
    "            self\n",
    "        \n",
    "        \"\"\"\n",
    "        self.dfw = noaa.noaa_from_web(self.isd_code, start, end).fillna(method='backfill')\n",
    "            \n",
    "        # drop relative humididty \n",
    "        self.dfw = self.dfw.drop('RHPeriod', axis = 1)\n",
    "        \n",
    "        # replace null values\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].replace(9999,np.nan)\n",
    "        self.dfw['Pressure'] = self.dfw['Pressure'].replace(99999,np.nan)\n",
    "        self.dfw['Humidity'] = self.dfw['Humidity'].replace(999, np.nan)\n",
    "        self.dfw['Sky'] = self.dfw['Sky'].replace([9,99], np.nan)\n",
    "        \n",
    "        # scale values back\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].map(lambda x : x/10)\n",
    "        self.dfw['Pressure'] = self.dfw['Pressure'].map(lambda x : x/10)\n",
    "        \n",
    "        # map sky oktas to coverage percentages, roughly\n",
    "        self.dfw['Sky'] = self.dfw['Sky'].map(lambda x : x/8)\n",
    "        \n",
    "        # convert C to F\n",
    "        self.dfw['Temperature'] = self.dfw['Temperature'].map(lambda x : x * 9/5 + 32)\n",
    "        self.weather = self.dfw\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def merge_dfs(self, start='2016-01-01', end='2017-01-01'):\n",
    "        \"\"\"Merges violent crime and weather into a single dataset, df, cut into a range\n",
    "        \n",
    "        Args:\n",
    "            start (date string): Start date to cut\n",
    "            end (date string): End date to cut to\n",
    "            \n",
    "        Returns:\n",
    "            Self\n",
    "        \n",
    "        \"\"\"\n",
    "        self.df = self.dfw.join(self.dfv, how='outer')\n",
    "        self.df = self.df.groupby( \n",
    "                        [self.df.index.year, \n",
    "                         self.df.index.month, \n",
    "                         self.df.index.day, \n",
    "                         self.df.index.hour]\n",
    "                    ).agg ({   \n",
    "                         'Temperature' : 'mean', \n",
    "                         'Pressure' : 'mean',\n",
    "                         'Humidity' : 'mean',\n",
    "                         'Sky' : 'mean',\n",
    "                         'Crime' : 'count'}\n",
    "                    ).reset_index().rename(columns={\n",
    "                        'level_0':'year',\n",
    "                        'level_1':'month',\n",
    "                        'level_2':'day',\n",
    "                        'level_3':'hour',\n",
    "                    })\n",
    "        s = pd.to_datetime(self.df[['year', 'month', 'day', 'hour']])\n",
    "        self.df = self.df.set_index(s).drop(['year', 'month', 'day', 'hour'], \n",
    "                                    axis=1)\n",
    "        \n",
    "        self.df = self.df.loc[self.df.index > start]\n",
    "        self.df = self.df.loc[self.df.index < end]\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A dictionary to hold all the city classes, initialized but not loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = {\n",
    "    ##'Albany, GA': City('Albany, GA', '../data/crime_albany_ga.csv.gz', '722160-13869'),\n",
    "    ## Not enough data for Albany\n",
    "    'Baltimore, MD': City('Baltimore, MD', '../data/crime_baltimore_md.csv.gz', '745944-93784'),\n",
    "    'Detroit, MI': City('Detroit, MI', '../data/crime_detroit_mi.csv.gz', '725375-14822'),\n",
    "    'Flint, MI': City('Flint, MI', '../data/crime_flint_mi.csv.gz', '726370-14826'),\n",
    "    'Memphis, TN': City('Memphis, TN', '../data/crime_memphis_tn.csv.gz', '723340-13893'),\n",
    "    'Philadelphia, PA': City('Philadelphia, PA', '../data/crime_philadelphia_pa.csv.gz', '724080-13739'),\n",
    "    ##'Pine Bluff AR': \n",
    "    ## no data for Pine Bluff\n",
    "    'St. Louis, MO': City('St. Louis, MO', '../data/crime_st.+louis_mo.csv.gz', '725314-03960'),\n",
    "    'Toledo, OH': City('Toledo, OH', '../data/crime_toledo_oh.csv.gz', '720275-04872')\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the crime data into the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for city in cities.values():\n",
    "    city.load_crime().load_weather().merge_dfs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis <a class=\"anchor\" id=\"analysis\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've loaded all out data, we can begin to analyse the data. First let's define some hypotheses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses <a class=\"anchor\" id=\"hypothesis\"></a>\n",
    "* Temperature is positively correlated with the violent crime rates (Richard)\n",
    "* Humidity and pressure have no impact on violent crime\n",
    "* Violent crime is higher in summer vs winter (Lalo)\n",
    "* Violent crime is higher during midnight hours (11:00pm - 2:00am) (Lexi)\n",
    "* Is there a spike in total crime at 2:00AM because bars close? (Laxo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration <a class=\"anchor\" id=\"exploration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to explore the data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two ways to parse the time, by dropping non-listed times or converting them to default to 00:00. We chose to use by default dropping incomplete data, but we wanted to have a metric of how much data we lost.\n",
    "\n",
    "Below is a comparison of the amount of entries lost as a percentage of the whole for each city, along with the times required to parse it. Dropping dates is considerably faster.\n",
    "\n",
    "Because of the amount of lost data, we chose to particularly examine Philadelphia and Memphis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if PROCESS_FULLY:\n",
    "    datalosses = pd.DataFrame()\n",
    "\n",
    "    for city in cities.values():\n",
    "        print(city.name)\n",
    "\n",
    "        city.load_crime(False)\n",
    "        t1 = time.time()\n",
    "        city.process_crime(how='Slow')\n",
    "        t2 = time.time()\n",
    "        slow = t2 - t1\n",
    "        df1 = city.dfc\n",
    "\n",
    "        city.load_crime(False)\n",
    "        t1 = time.time()\n",
    "        city.process_crime(how='Fast')\n",
    "        t2 = time.time()\n",
    "        fast = t2-t1\n",
    "        df2 = city.dfc\n",
    "\n",
    "        print('Slow: {} Fast: {}'.format(slow, fast))\n",
    "        datalosses[city.name] = df2.Crime.value_counts()/df1.Crime.value_counts()\n",
    "\n",
    "    datalosses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a quick correlation of temperature on the X axis and violent crime incidences on the hour measured at that tempreature on the Y axis. There is somewhat of a positive correlation between the largest values in some cities, but it's completely swamped on the lower end by low values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for city in cities.values():\n",
    "    df = city.df[['Temperature', 'Crime']].dropna()\n",
    "    sns.jointplot(x='Temperature', y='Crime', data=df)\n",
    "    plt.title(city.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A distribution of the amounts of violent crimes in each city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for city in cities.values():\n",
    "    print(city.name)\n",
    "    city.dfc.Crime.value_counts().plot(kind='bar')\n",
    "    plt.title(city.name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps there is a correlation between the day of the week and criminal activity. Perhaps on the weekend, with more free time, criminals may become violent instead of working at their jobs.\n",
    "\n",
    "There does not appear to be a correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phili = cities['Philadelphia, PA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a hack\n",
    "d = {'Monday' : 0, 'Tuesday' : 1, 'Wednesday' : 2, 'Thursday' : 3, 'Friday': 4 , 'Saturday' : 5, 'Sunday' : 6}\n",
    "\n",
    "# Philadelphia, PA\n",
    "df4 = phili.dfv\n",
    "days = {}\n",
    "for val in df4.index:\n",
    "    day = calendar.day_name[val.weekday()]\n",
    "    days[day] = days.get(day, 0) + 1\n",
    "sorted_days = [w for w in sorted(days.items(), key=lambda x: d[x[0]])]\n",
    "plt.bar(range(len(days)), [day[1] for day in sorted_days], align = 'center', color = 'b')\n",
    "plt.xticks(range(len(days)), [day[0] for day in sorted_days])\n",
    "plt.title('Philadelphia, PA Crimes per Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Crimes per Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be worthwhile to examine the total crime, and not violent crime for weekend activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a hack\n",
    "d = {'Monday' : 0, 'Tuesday' : 1, 'Wednesday' : 2, 'Thursday' : 3, 'Friday': 4 , 'Saturday' : 5, 'Sunday' : 6}\n",
    "\n",
    "# Philadelphia, PA\n",
    "df4 = phili.dfc\n",
    "days = {}\n",
    "for val in df4.index:\n",
    "    day = calendar.day_name[val.weekday()]\n",
    "    days[day] = days.get(day, 0) + 1\n",
    "sorted_days = [w for w in sorted(days.items(), key=lambda x: d[x[0]])]\n",
    "plt.bar(range(len(days)), [day[1] for day in sorted_days], align = 'center', color = 'b')\n",
    "plt.xticks(range(len(days)), [day[0] for day in sorted_days])\n",
    "plt.title('Philadelphia, PA Crimes per Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Total Crimes per Day')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There appears to be a tendency to commit crimes during the week. Since this includes burglary and theft, this may be because criminals are more likely to commit crimes when they know their victim are away at work or school."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 1: Violent crime is correlated with higher temperatures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to see how violent crime is correlated with temperature. We can find the mean temperature of a city, and posit that when the temperature is unusually high (greater than 1 standard deviation), the amount of crimes are also higher.\n",
    "\n",
    "* let p = the average crimes per hour in a city in a sample with higher temperature than average\n",
    "* let mu be the average crimes per hour over a year\n",
    "* let h_0 : p = mu and h_a : p > mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = phili.df.Crime.mean()\n",
    "n = 100\n",
    "p0 = mu\n",
    "stdtemp = phili.df.Temperature.std()\n",
    "tempmu = phili.df.Temperature.mean()\n",
    "alpha = 0.01\n",
    "\n",
    "# Select only days which are greater than 1 std from the mean\n",
    "sample = phili.df.Crime[phili.df.Temperature > tempmu + 1 * stdtemp].sample(n, random_state=57)\n",
    "p = sample.mean()\n",
    "\n",
    "S = stats.tstd(sample)\n",
    "z = (p-p0)/(S/np.sqrt(n))\n",
    "z_alpha = stats.norm().ppf(1-alpha)\n",
    "\n",
    "\n",
    "if z > z_alpha:\n",
    "    print(\"Since Z={0:4.2f} > Z_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_alpha))\n",
    "else :\n",
    "    print(\"Since Z={0:4.2f} < Z_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about for abnormally low temperatures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = phili.df.Crime.mean()\n",
    "n = 100\n",
    "p0 = mu\n",
    "stdtemp = phili.df.Temperature.std()\n",
    "tempmu = phili.df.Temperature.mean()\n",
    "alpha = 0.01\n",
    "\n",
    "# Select only days which are greater than 1 std from the mean\n",
    "sample = phili.df.Crime[phili.df.Temperature > tempmu - 1 * stdtemp].sample(n, random_state=57)\n",
    "p = sample.mean()\n",
    "\n",
    "S = stats.tstd(sample)\n",
    "z = (p-p0)/(S/np.sqrt(n))\n",
    "z_alpha = stats.norm().ppf(1-alpha)\n",
    "\n",
    "\n",
    "if z > z_alpha:\n",
    "    print(\"Since Z={0:4.2f} > Z_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_alpha))\n",
    "else :\n",
    "    print(\"Since Z={0:4.2f} < Z_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 2: Violent crime is not correlated with humidity or pressure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to compare violent crime with abnormal pressure\n",
    "\n",
    "* let p = the average crimes per hour in a city in a sample with higher or lower pressure than average\n",
    "* let mu be the average crimes per hour over a year\n",
    "* let h_0 : p = mu and h_a : p > mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = phili.df.Crime.mean()\n",
    "n = 100\n",
    "p0 = mu\n",
    "stdpressure = phili.df.Pressure.std()\n",
    "pressuremu = phili.df.Pressure.mean()\n",
    "alpha = 0.01\n",
    "half_alpha = alpha/2\n",
    "\n",
    "# Select only days which are greater than 1 std from the mean\n",
    "sample = phili.df.Crime[(phili.df.Pressure > pressuremu + 1 * stdpressure) | (phili.df.Pressure < mu - 1 * stdpressure)].sample(n, random_state=57)\n",
    "p = sample.mean()\n",
    "\n",
    "S = stats.tstd(sample)\n",
    "z = (p-p0)/(S/np.sqrt(n))\n",
    "z_half_alpha = stats.norm().ppf(1-half_alpha)\n",
    "\n",
    "\n",
    "if np.abs(z) > z_half_alpha:\n",
    "    print(\"Since |Z|={0:4.2f} > Z_half_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_half_alpha))\n",
    "else :\n",
    "    print(\"Since |Z|={0:4.2f} < Z_half_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_half_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about humidity?\n",
    "\n",
    "* let p = the average crimes per hour in a city in a sample with higher humidity than average\n",
    "* let mu be the average crimes per hour over a year\n",
    "* let h_0 : p = mu and h_a : p > mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = phili.df.Crime.mean()\n",
    "n = 100\n",
    "p0 = mu\n",
    "stdhu = phili.df.Humidity.std()\n",
    "humu = phili.df.Humidity.mean()\n",
    "alpha = 0.01\n",
    "\n",
    "# Select only days which are greater than 1 std from the mean\n",
    "sample = phili.df.Crime[phili.df.Temperature > humu + 1 * stdhu].sample(n, random_state=57)\n",
    "p = sample.mean()\n",
    "\n",
    "S = stats.tstd(sample)\n",
    "z = (p-p0)/(S/np.sqrt(n))\n",
    "z_alpha = stats.norm().ppf(1-alpha)\n",
    "\n",
    "\n",
    "if z > z_alpha:\n",
    "    print(\"Since Z={0:4.2f} > Z_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_alpha))\n",
    "else :\n",
    "    print(\"Since Z={0:4.2f} < Z_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the references suggested that dryer days correlate with more crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = phili.df.Crime.mean()\n",
    "n = 100\n",
    "p0 = mu\n",
    "stdhu = phili.df.Humidity.std()\n",
    "humu = phili.df.Humidity.mean()\n",
    "alpha = 0.01\n",
    "\n",
    "# Select only days which are greater than 1 std from the mean\n",
    "sample = phili.df.Crime[phili.df.Temperature < humu - 1 * stdhu].sample(n, random_state=57)\n",
    "p = sample.mean()\n",
    "\n",
    "S = stats.tstd(sample)\n",
    "z = (p-p0)/(S/np.sqrt(n))\n",
    "z_alpha = stats.norm().ppf(1-alpha)\n",
    "\n",
    "\n",
    "if z > z_alpha:\n",
    "    print(\"Since Z={0:4.2f} > Z_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_alpha))\n",
    "else :\n",
    "    print(\"Since Z={0:4.2f} < Z_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Hypothesis 3: Violent crime is higher in summer than in winter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = blt.df[['Temperature', 'Pressure', 'Crime']] # get df\n",
    "df.corr().plot(kind = 'box', title = 'Baltimore, MD') # create graph of corr\n",
    "df.corr().plot(kind = 'bar', title = 'Baltimore, MD') # create graph of corr\n",
    "df.corr().plot(kind = 'line', title = 'Baltimore, MD') # create graph of corr\n",
    "print('Baltimore, MD', df.corr()) # print chart of corr\n",
    "df2 = stl.df[['Temperature', 'Pressure', 'Crime']] # get df \n",
    "df2.corr().plot(kind = 'box', title = 'St. Louis, MO') # create graph of corr\n",
    "df2.corr().plot(kind = 'bar', title = 'St. Louis, MO') # create graph of corr\n",
    "df2.corr().plot(kind = 'line', title = 'St. Louis, MO') # create graph of corr\n",
    "print('St. Louis, MO', df2.corr()) # print chart of corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blt = cities['Baltimore, MD']\n",
    "blt.load_crime().load_weather().merge_dfs()\n",
    "seasonalDFV = blt.dfv\n",
    "\n",
    "springStartMonth = 3\n",
    "springEndMonth = 5\n",
    "\n",
    "summerStart = 6\n",
    "summerEnd = 9\n",
    "fallStart = 10\n",
    "fallEnd = 11\n",
    "winterStart =12\n",
    "winterEnd = 3\n",
    "\n",
    "\n",
    "SpringMask = (seasonalDFV.index.month >= springStartMonth) & (seasonalDFV.index.month <= springEndMonth) \n",
    "summerMask =(seasonalDFV.index.month >= summerStart ) & (seasonalDFV.index.month <= summerEnd)\n",
    "fallMask= (seasonalDFV.index.month >= fallStart) & (seasonalDFV.index.month <= fallEnd)\n",
    "wintMask = (seasonalDFV.index.month < winterEnd) | (seasonalDFV.index.month >= winterStart) \n",
    "\n",
    "winterDF = pd.DataFrame(seasonalDFV.loc[wintMask].Crime.value_counts())\n",
    "springDF = pd.DataFrame(seasonalDFV.loc[SpringMask].Crime.value_counts())\n",
    "summerDF = pd.DataFrame(seasonalDFV.loc[summerMask].Crime.value_counts())\n",
    "fallDF = pd.DataFrame(seasonalDFV.loc[fallMask].Crime.value_counts())\n",
    "\n",
    "#winterDF = winterDF.drop('Arson')\n",
    "springDF = springDF.drop('Arson')\n",
    "summerDF = summerDF.drop('Arson')\n",
    "fallDF = fallDF.drop('Arson')\n",
    "\n",
    "winterDF = winterDF.rename(columns={'Crime':'Winter'})\n",
    "summerDF = summerDF.rename(columns={'Crime':'Summer'})\n",
    "springDF = springDF.rename(columns={'Crime':'Spring'})\n",
    "fallDF = fallDF.rename(columns={'Crime':'Fall'})\n",
    "\n",
    "fallDF = fallDF.merge(winterDF, left_index=True, right_index=True,how='left')\n",
    "springDF = springDF.merge(summerDF,left_index=True,right_index=True,how='left')\n",
    "springDF = springDF.merge(fallDF,left_index=True,right_index=True,how='left')\n",
    "crimesCountBySeason = springDF\n",
    "crimesCountBySeason.index.name = 'Crime'\n",
    "\n",
    "crimesCountBySeason.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phili = cities['Philadelphia, PA']\n",
    "phili.load_crime().load_weather().merge_dfs()\n",
    "seasonalDFV = phili.dfv\n",
    "\n",
    "springStartMonth = 3\n",
    "springEndMonth = 5\n",
    "\n",
    "summerStart = 6\n",
    "summerEnd = 9\n",
    "fallStart = 10\n",
    "fallEnd = 11\n",
    "winterStart =12\n",
    "winterEnd = 3\n",
    "\n",
    "\n",
    "SpringMask = (seasonalDFV.index.month >= springStartMonth) & (seasonalDFV.index.month <= springEndMonth) \n",
    "summerMask =(seasonalDFV.index.month >= summerStart ) & (seasonalDFV.index.month <= summerEnd)\n",
    "fallMask= (seasonalDFV.index.month >= fallStart) & (seasonalDFV.index.month <= fallEnd)\n",
    "wintMask = (seasonalDFV.index.month < winterEnd) | (seasonalDFV.index.month >= winterStart) \n",
    "\n",
    "winterDF = pd.DataFrame(seasonalDFV.loc[wintMask].Crime.value_counts())\n",
    "springDF = pd.DataFrame(seasonalDFV.loc[SpringMask].Crime.value_counts())\n",
    "summerDF = pd.DataFrame(seasonalDFV.loc[summerMask].Crime.value_counts())\n",
    "fallDF = pd.DataFrame(seasonalDFV.loc[fallMask].Crime.value_counts())\n",
    "\n",
    "#winterDF = winterDF.drop('Arson')\n",
    "springDF = springDF.drop('Arson')\n",
    "summerDF = summerDF.drop('Arson')\n",
    "fallDF = fallDF.drop('Arson')\n",
    "\n",
    "winterDF = winterDF.rename(columns={'Crime':'Winter'})\n",
    "summerDF = summerDF.rename(columns={'Crime':'Summer'})\n",
    "springDF = springDF.rename(columns={'Crime':'Spring'})\n",
    "fallDF = fallDF.rename(columns={'Crime':'Fall'})\n",
    "\n",
    "fallDF = fallDF.merge(winterDF, left_index=True, right_index=True,how='left')\n",
    "springDF = springDF.merge(summerDF,left_index=True,right_index=True,how='left')\n",
    "springDF = springDF.merge(fallDF,left_index=True,right_index=True,how='left')\n",
    "crimesCountBySeason = springDF\n",
    "crimesCountBySeason.index.name = 'Crime'\n",
    "\n",
    "crimesCountBySeason.plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 4: violent crime is higher during the midnight hours of 11:00PM to 2:00AM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the distribution based on the hour of Philadelphia. Are the crimes higher during midnight hours?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phili.dfv.Crime.groupby(phili.dfv.index.hour).count().plot(kind='bar')\n",
    "plt.title(\"Philadelphia hourly violent crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = phili.dfv.between_time(start_time='23:00', end_time='23:59')\n",
    "dff = phili.dfv.between_time(start_time = '00:00', end_time = '02:00')\n",
    "df3 = phili.dfv.between_time(start_time = '02:01', end_time = '22:59')\n",
    "df4 = phili.dfv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phMidHo = df.groupby(df.index.hour).count().sum() + dff.groupby(dff.index.hour).count().sum()\n",
    "phOtherHo = df3.groupby(df3.index.hour).count().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mMidHo = df.groupby(df.index.hour).count().sum() + df2.groupby(df2.index.hour).count().sum()\n",
    "mOtherHo = df3.groupby(df3.index.hour).count().sum()\n",
    "print(\"Philadelphia Midnight Hours : {}, Memphis Midnight Hours : {}\".format(phMidHo[0], mMidHo[0]))\n",
    "print(\"Philadelphia All Other Hours : {}, Memphis All Other Hours : {}\".format(phOtherHo[0], mOtherHo[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is no correlation, there should be an equal number of crime at all hours. Because the midnight hours are 3 in total, they should occupy $$\\frac{3}{24}$$ of the total crimes\n",
    "\n",
    "Let p = percentage of crimes committed during the midnight hours\n",
    "\n",
    "Let H_0 : p = 1/6, and H_a : p > 1/6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 1000\n",
    "p0 = 1/6\n",
    "\n",
    "sample = phili.dfv.sample(n, random_state=31)\n",
    "midnight = sample.between_time(start_time='23:00', end_time='23:59')\n",
    "midnight2 = sample.between_time(start_time='0:00', end_time='2:00')\n",
    "midnight = midnight.count() + midnight2.count()\n",
    "\n",
    "pHat = midnight.sum()/n\n",
    "sigma = np.sqrt((p0 * (1-p0)/n))\n",
    "mu = p0\n",
    "\n",
    "z = (pHat-p0)/sigma\n",
    "alpha = 0.01\n",
    "z_alpha = stats.norm().ppf(1-alpha)\n",
    "\n",
    "if z > z_alpha:\n",
    "    print(\"Since Z={0:4.2f} > Z_alpha={1:4.2f}, in Philadelpha can we reject H_0\".format(z, z_alpha))\n",
    "else :\n",
    "    print(\"Since Z={0:4.2f} < Z_alpha={1:4.2f}, in Philadelpha we cannot reject H_0\".format(z, z_alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis 5: There is an increase in crimes committed during the time that bars close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There doesn't appear to be any correlation between the bar closing time and incidence of crime in Philadelphia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = phili.dfc.between_time(start_time = '01:00', end_time = '03:00')\n",
    "df.groupby(df.index.hour).count().plot(kind = 'bar', title = 'Philadelphia, PA', color = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results and Conclusions <a class=\"anchor\" id=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Back to Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
