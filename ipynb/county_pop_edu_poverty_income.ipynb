{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import sys, os\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cd = os.path.split(os.getcwd())[0]\n",
    "if cd not in sys.path:\n",
    "    sys.path.append(cd)\n",
    "\n",
    "from lib import noaa, bexarcrime\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[sauce A](https://www.ers.usda.gov/data-products/county-level-data-sets/county-level-data-sets-download-data/)\n",
    "\n",
    "[sauce B](http://www.icpsr.umich.edu/icpsrweb/NACJD/studies/35019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# using crime reports, not arrests \n",
    "crime = pd.read_csv('../data/CountyCrimeReports.tsv', sep='\\t')\n",
    "crime['FIPS'] = crime['FIPS_ST'] * 1000 + crime['FIPS_CTY']\n",
    "crime['vcrime'] = crime['MURDER'] + crime['RAPE'] + crime['ROBBERY'] + crime['AGASSLT']\n",
    "crime = crime.set_index('FIPS')\n",
    "crime = crime[['COVIND', 'vcrime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "edu = pd.read_excel('../data/Education.xls', skiprows=4)\n",
    "\n",
    "# state and areas are named nicely in this dataset and will be kept for the later 'join'\n",
    "# columns[-4:] include most recent data for adults eduction\n",
    "# I chose the most recent because its not like the total number of HS dropouts is going to change THAT much\n",
    "edu = edu[['FIPS Code', 'State', 'Area name'] + list(edu.columns[-4:])]\n",
    "edu.rename(columns={'FIPS Code':'FIPS', \\\n",
    "                    'Area name':'County',\\\n",
    "                    'Percent of adults with less than a high school diploma, 2011-2015':'p_no_HS_dip', \\\n",
    "                    'Percent of adults with a high school diploma only, 2011-2015':'p_HS_dip',\\\n",
    "                    'Percent of adults completing some college or associate\\'s degree, 2011-2015':'p_some_college',\\\n",
    "                    'Percent of adults with a bachelor\\'s degree or higher, 2011-2015':'p_college_dip'}, inplace=True)\n",
    "edu = edu.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pop = pd.read_excel('../data/PopulationEstimates.xls', skiprows=2)\n",
    "\n",
    "# average the columns\n",
    "cols = ['POP_ESTIMATE_2010','POP_ESTIMATE_2011','POP_ESTIMATE_2012','POP_ESTIMATE_2013','POP_ESTIMATE_2014','POP_ESTIMATE_2015','POP_ESTIMATE_2016']\n",
    "pop['avgpop'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "# more averaging\n",
    "cols = ['N_POP_CHG_2010','N_POP_CHG_2011','N_POP_CHG_2012','N_POP_CHG_2013','N_POP_CHG_2014','N_POP_CHG_2015','N_POP_CHG_2016']\n",
    "pop['dpop/dt'] = pop[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "# only pull FIPS code, population, and dp\n",
    "pop = pop[['FIPS', 'avgpop', 'dpop/dt']]\n",
    "pop = pop.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pov = pd.read_excel('../data/PovertyEstimates.xls', skiprows=3)\n",
    "# only select poverty percentage\n",
    "pov = pov[['FIPStxt', 'PCTPOVALL_2015']]\n",
    "pov.rename(columns={'FIPStxt':'FIPS', 'PCTPOVALL_2015':'p_impoverished'}, inplace=True)\n",
    "pov = pov.set_index('FIPS')\n",
    "pov.p_impoverished = pd.to_numeric(pov.p_impoverished, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emp = pd.read_excel('../data/Unemployment.xls', skiprows=9)\n",
    "\n",
    "#avg unemployment\n",
    "cols = ['Unemployment_rate_2007', 'Unemployment_rate_2008', 'Unemployment_rate_2009', 'Unemployment_rate_2010', 'Unemployment_rate_2011', 'Unemployment_rate_2012', 'Unemployment_rate_2013', 'Unemployment_rate_2014', 'Unemployment_rate_2015', 'Unemployment_rate_2016']\n",
    "emp['p_unempl'] = emp[cols].sum(axis=1) / len(cols)\n",
    "\n",
    "#only pull average and income\n",
    "emp = emp[['FIPStxt', 'p_unempl', 'Median_Household_Income_2015']]\n",
    "emp.rename(columns={'FIPStxt':'FIPS', 'Median_Household_Income_2015':'med_income'}, inplace=True)\n",
    "emp = emp.set_index('FIPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = edu.join([pop,pov,emp,crime], how='outer')\n",
    "df = df.where(df.State != 'PR').dropna(how='all') ## Puerto Rico has unreliable data\n",
    "\n",
    "#pull out nationwide data\n",
    "us = df.iloc[0]\n",
    "df = df.drop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pull out statewide data\n",
    "s = [x for x in range(1000,75000,1000)]\n",
    "states = df.loc[s].dropna(how='all')\n",
    "\n",
    "# all thats left is county level data\n",
    "df = df.drop(states.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "df['p_dpop'] = df['dpop/dt']/df['avgpop']\n",
    "df['vcrime_rate'] = 100000 * df['vcrime']/df['avgpop']\n",
    "df = df.drop(['dpop/dt', 'vcrime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# According to the 2008 presidential election\n",
    "blue_states =['WA', 'OR', 'CA', 'NV', 'NM', 'CO', 'MN', 'IA', 'WI', 'IL', 'IN', 'MI', 'OH', 'PA', 'NY', 'VT', 'NH', 'ME', 'MA', 'CT', 'RI', 'NJ', 'DE', 'MD', 'VA', 'NC', 'FL', 'HI']\n",
    "red_states = ['ID', 'MT', 'WY', 'UT', 'AZ', 'ND', 'SD', 'NE', 'KS', 'OK', 'TX', 'MO', 'AR', 'LA', 'WV', 'KY', 'TN', 'MS', 'AL', 'GA', 'SC', 'AK']\n",
    "fix, ax = plt.subplots(figsize=(20,10))\n",
    "pal = {state: 'r' if state in red_states else \"b\" for state in df.State}\n",
    "sns.boxplot(ax=ax, x='State', y='vcrime_rate', data=df, palette=pal)\n",
    "#sns.boxplot(ax=ax2, x='State', y='vcrime_rate', data=df.where(df.State.isin(red_states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, ax = plt.subplots(figsize=(20,10))\n",
    "sns.boxplot(ax=ax, x='State', y='p_no_HS_dip', data=df, palette=pal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Graphs of factors to violent crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df, y_vars=['vcrime_rate'], x_vars=['p_no_HS_dip', 'p_HS_dip', 'p_some_college', 'p_college_dip', 'avgpop',\n",
    "       'p_impoverished', 'p_unempl', 'med_income', 'p_dpop', 'vcrime_rate'], dropna=True, size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent crime per 100,000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.avgpop.dropna().apply(np.log10), axlabel=\"Population (log10)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.p_unempl.dropna(), axlabel='Unemployment Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.p_impoverished.dropna(), axlabel=\"Poverty Rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bins data into high, medium, and low (based on national quantiles) for grouping\n",
    "binned = pd.DataFrame({c : pd.qcut(df[c], 3, labels=['L', 'M', 'H']) for c in df.drop(['State', 'County', 'COVIND'], axis=1).columns}).join(df[['State', 'County', 'COVIND']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apparently the worst counties to be in in Texas\n",
    "# High violent crime rate, high rate of unemployment, and high populations\n",
    "TX = binned.dropna(how='all').groupby(['vcrime_rate', 'p_unempl', 'avgpop'])\n",
    "df.loc[TX.get_group(('H', 'H', 'H')).index].where(df.State == 'TX').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df.where(df.State=='TX').vcrime_rate.dropna(), label=\"Violent Crime Rates in Texas\")\n",
    "sns.distplot(df.vcrime_rate.dropna(), axlabel=\"Violent Crime Rates in US and Texas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#highest crime counties in US\n",
    "# note that high city crime does not necessarily match high county crime\n",
    "# eg: chicago is high crime, but it's split between 2 counties\n",
    "# St Louis has the highest crime, but it's its own county, so it tops this list as well\n",
    "df.where(df.avgpop > 10000).sort_values('vcrime_rate', ascending=False)[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group the data by violent crime rate, poverty rate, unemployment rate, and population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = ['vcrime_rate', 'p_impoverished', 'p_unempl', 'avgpop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# all counties grouped by H/M/L rates of whatever\n",
    "c = binned.dropna(how='all').groupby(groups[::-1])\n",
    "c.count().where(c.count().State > 10).dropna().sort_values('State', ascending=False)['State'].unstack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select counties with high rates of enemployment, violent crime, poverty, and large populations sampled using a nonrandom seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the original values of the first row ^^^\n",
    "# such that cities have high crime rate, high poverty, High unemployment, High population, etc\n",
    "selection = ('H','H','H', 'H')\n",
    "for x in groups:\n",
    "    print(\"%10s \" %x[:10], end='')\n",
    "print('')\n",
    "for x in selection:\n",
    "    print(\"%10s \" %x[:10], end='')\n",
    "HHHstates = df.loc[c.get_group(selection).index]\n",
    "HHHstates.where(HHHstates.vcrime_rate > 800).dropna().sample(10, random_state=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From those counties, we selected the county seats as the cities\n",
    "We looked up the latitude and longitude of them to match to NOAA's list of weather stations. We used Pythagoras' theorem to find the closest station to the city, as some cities may not have one within city limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cities = {\n",
    "    'Philidelphia, PA' : (39.9526, -75.1652), #Philadelphia County\n",
    "    'Albany, GA' : (31.5785, -84.1557), # Gougherty County\n",
    "    'Memphis, TN' : (35.1495, -90.0490), # Shelby County and Crittenden County\n",
    "    'Toledo, OH' : (41.6639, -83.5552), # Lucas County\n",
    "    'Pine Bluff AR' : (34.2284, -92.0032), # Jefferson County\n",
    "    'Detroit, MI' : (42.3314, -83.0458), # Wayne County\n",
    "    'Baltimore, MD' : (39.2904, -76.6122), # Baltimore City\n",
    "    'Flint, MI' : (43.0125, -83.6875), # Genesee County\n",
    "    'St. Louis, MO' : (38.6270, -90.1994) # St. Louis City\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# list of stations with location, name, and recording beginning and end dates\n",
    "hist = pd.read_csv('ftp://ftp.ncdc.noaa.gov/pub/data/noaa/isd-history.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# only recent stations\n",
    "hist = hist.where(hist.END > 20120101 ).dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(a, b):\n",
    "    return math.sqrt((a[0]-b[0])**2 + (a[1]-b[1])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_station_code(usaf, wban):\n",
    "    usafstr = str(int(usaf))\n",
    "    wbanstr = str(int(wban))\n",
    "    \n",
    "    if len(usafstr) < 6:\n",
    "        usafstr = '0'*(6-len(usafstr)) + usafstr\n",
    "        \n",
    "    if len(wbanstr) < 5:\n",
    "        wbanstr = '0'*(5-len(wbanstr)) + wbanstr\n",
    "        \n",
    "    return usafstr + '-' + wbanstr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stations = dict()\n",
    "for city in cities.keys():\n",
    "    coord = cities[city]\n",
    "    mindist = 999\n",
    "    minindex = 0\n",
    "    for index, row in hist.iterrows():\n",
    "        d = dist(coord, (row['LAT'], row['LON']))\n",
    "        if (d < mindist):\n",
    "            mindist = d\n",
    "            minindex = index\n",
    "    print('Nearest ({:^6.2f}) ISD to {:20} is {:40} at loc {}'.format(mindist, city, hist.loc[minindex]['STATION NAME'], minindex))\n",
    "    stations[city] = format_station_code(hist.loc[minindex]['USAF'], hist.loc[minindex]['WBAN'])\n",
    "    print('\\tStation code is {}'.format(stations[city]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
